# LLM_agent_DLS_spring_2025
My LangChain LLM agent as diploma project for DLS (2nd semester, spring 2025)

Я реализовал LLM-фгента при помощи LangGraph и протестировал его на задаче оценки релевантности организаций пользовательским запросам.
Весь необходимый для запуска агента код находится в приложенном ноутбуке.
Для запуска также потребуются API-ключи tavily и сервиса Vse_GPT.

**Схема графа:** (точками обозначены связи, между которыми происходит выбор, звездочками - фиксированные связи)
                     +-----------+                 
                     | __start__ |                 
                     +-----------+                 
                           *                       
                           *                       
                           *                       
                      +--------+                   
                      | decide |*                  
                   ...+--------+ ****              
               ....        .         ****          
           ....           .              ***       
         ..               .                 ****   
+---------+          +--------+                 ** 
| __end__ |          | search |                  +-----------+ 
+---------+          +--------+                  | summarize | 
                          *                      +-----------+ 
                          *                  ****     
                          *              ****        
                    +-----------+    ****             
                    | read_urls |****                   
                    +-----------+                  

**Описание алгоритма действий агента:**

1) Информация поступает в решающую сеть decide
2) Если decide считает, что имеющейся информации недостаточно, то она отвечает текстом поискового запроса и направляет его в модуль search
3) Search отправляет запрос черех tavily и возвращает первые две ссылки из поиска
4) Текст по этим ссылкам затем считывает функция read_urls. Она работает через request, а если request не считывает текст, то подключает selenium
5) Считанный текст отправляется в модуль summarize, где модель пишет короткие саммари (до 1000 символов) на основе этого текста
6) Саммари всесте со всей остальной информацией возвращаются в decide, которая может либо еще один раз совершить поиск (возможно максимум 2 поиска для одного объекта), либо дать финальный ответ: 0.0 либо 1.0

В качестве моделей для саммаризации и принятия решений я использовал модель deepseek-chat-0324-alt-fast, т.к. она недорогая, быстрая и при этом достаточно мощная. Температуру я взял 0.1.
Baseline я построил при помощи этой же модели без дополнительных функций. Температура для baseline была 0.0.

**Результаты:**

Baseline (одиночный запрос в deepseek-chat-0324-alt-fast):
Accuracy: 0.668
F1 score: 0.711

Агент:
Agent accuracy: 0.66
Agent F1 score: 0.68

К сожалению, агент показал себя несколько хуже, чем базовая линия. По точности результаты сравнимы, но по F1 агент ощутимо отстал.
Возможно, причиной этому стал неоптимальный промпт агента или недостаточно хороший модуль по парсингу веб-страниц,
который только считывает тест веб-страницы без навигации по сайту и не умеет обходить блокировки.
