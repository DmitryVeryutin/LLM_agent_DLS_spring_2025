{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        " Я реализовал LLM-фгента при помощи LangGraph и протестировал его на задаче оценки релевантности организаций пользовательским запросам. Для запуска нужно вручную загрузить файл с данными data_final_for_dls.jsonl в папку content, а также добавить в секреты блокнота свои API-ключи поисковика tavily и сервиса Vse_GPT."
      ],
      "metadata": {
        "id": "n1p0OjoVHqBh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Подготовка"
      ],
      "metadata": {
        "id": "T4Qe7A5lhiOO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "_qaSV48cX-qB"
      },
      "outputs": [],
      "source": [
        "! pip -q install ollama langgraph langchain duckduckgo-search tavily-python langchain_community tqdm requests openai beautifulsoup4 selenium chardet requests scikit-learn ddgs graphviz grandalf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download Chromium and Chromedriver v114\n",
        "!wget -q https://storage.googleapis.com/chrome-for-testing-public/114.0.5735.90/linux64/chrome-linux64.zip\n",
        "!wget -q https://chromedriver.storage.googleapis.com/114.0.5735.90/chromedriver_linux64.zip\n",
        "\n",
        "# Extract both archives\n",
        "!unzip -q chrome-linux64.zip\n",
        "!unzip -q chromedriver_linux64.zip\n",
        "\n",
        "# Move to proper locations\n",
        "!mv chrome-linux64 /opt/chrome\n",
        "!mv chromedriver /opt/chrome/chromedriver\n",
        "!chmod +x /opt/chrome/chromedriver\n",
        "!ln -sf /opt/chrome/chromedriver /usr/bin/chromedriver"
      ],
      "metadata": {
        "id": "uOv1oEzswbr0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81be03da-d73a-4ea1-f09e-0c234f8f83ad"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "replace LICENSE.chromedriver? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ollama\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import langchain\n",
        "import langchain_community\n",
        "import langgraph\n",
        "from duckduckgo_search import DDGS\n",
        "import time\n",
        "\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain_community.chat_models import ChatOllama\n",
        "from duckduckgo_search import DDGS\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.firefox.service import Service\n",
        "from selenium.webdriver.firefox.options import Options\n",
        "\n",
        "import chardet\n",
        "from tavily import TavilyClient\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import HumanMessage\n",
        "import os\n",
        "from google.colab import userdata\n",
        "import pickle\n",
        "from IPython.display import clear_output\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score"
      ],
      "metadata": {
        "id": "lAakFweKsuIT"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Загрузка данных"
      ],
      "metadata": {
        "id": "qy0SPocmHf5m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Здесь требуется загрузать файл с данными data_final_for_dls.jsonl в папку content вручную.\n"
      ],
      "metadata": {
        "id": "ODCUQscgHO5k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_json(path_or_buf=\"/content/data_final_for_dls.jsonl\", lines=True)"
      ],
      "metadata": {
        "id": "j7rVj2F9uI0l"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = data[570:]\n",
        "eval_data = data[:570]\n",
        "eval_data = eval_data[eval_data[\"relevance\"] != 0.1]"
      ],
      "metadata": {
        "id": "lnYfLiI7uSN9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline"
      ],
      "metadata": {
        "id": "uKLOQZrVhqg2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Vse_GPT_key = userdata.get('Vse-GPT_key')\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    openai_api_key=Vse_GPT_key,\n",
        "    openai_api_base=\"https://api.vsegpt.ru/v1\",\n",
        "    model_name=\"deepseek/deepseek-chat-0324-alt-fast\",  # or deepseek-coder-instruct on site deepseek-reasoner\n",
        "    temperature=0.0\n",
        ")"
      ],
      "metadata": {
        "id": "5YMG5LUDhu29"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 0.518900 rub for IFNS request no-sale time\n",
        "pred_relevancies = []\n",
        "for index, organization in tqdm(eval_data.iterrows()):\n",
        "\n",
        "    prompt = f\"\"\"You are an agent who evaluates the relevancy of given organization to user request.\n",
        "    The user request is {organization['Text']}, organization data has name {organization['name'].split(';')[:2]},\n",
        "    address {organization['address']}, category {organization['normalized_main_rubric_name_ru']},\n",
        "    information about prices {organization['prices_summarized']} and summarized reviews {organization['reviews_summarized']}.\n",
        "    You must answer with only one number: 0.0 if organization is irrelevant to user requiest or 1.0 if organization is completely relevant to user requiest.\n",
        "    Only these two numbers (0.0 or 1.0) are allowed in answer. Your answer must contain only number, no additions allowed.\"\"\"\n",
        "\n",
        "    response = llm.invoke([HumanMessage(content=prompt)])\n",
        "\n",
        "    pred_relevancies.append(float(response.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8rtHzqSnGGC",
        "outputId": "288a5d6f-55d1-4509-fa58-aa474cae86f6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "500it [13:58,  1.68s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gt_relevancies = eval_data['relevance'].to_list()"
      ],
      "metadata": {
        "id": "tUGfY9ZXnvvF"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "print('Baseline accuracy:', accuracy_score(gt_relevancies, pred_relevancies))\n",
        "print('Baseline F1 score:', f1_score(gt_relevancies, pred_relevancies))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzibusJdbPfG",
        "outputId": "5068fff9-b5aa-41d0-e03b-4f7ba9eb0432"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline accuracy: 0.668\n",
            "Baseline F1 score: 0.710801393728223\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('baseline_eval.pkl', 'wb') as f:\n",
        "    pickle.dump(pred_relevancies, f)"
      ],
      "metadata": {
        "id": "oPJ3GDD3T0E-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agent"
      ],
      "metadata": {
        "id": "RZdJHHFIhvqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(\n",
        "    openai_api_key=Vse_GPT_key,\n",
        "    openai_api_base=\"https://api.vsegpt.ru/v1\",\n",
        "    model_name=\"deepseek/deepseek-chat-0324-alt-fast\",  # or deepseek-coder-instruct on site deepseek-reasoner\n",
        "    temperature=0.1\n",
        ")"
      ],
      "metadata": {
        "id": "yZkSOD6OuUsi"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_url_selenium(url: str) -> str:\n",
        "    '''\n",
        "    Функция принимает URL (str) и возвращает обработанный текст из html-файла (str) при помощи selenium\n",
        "    '''\n",
        "    try:\n",
        "        options = Options()\n",
        "        options.binary_location = \"/opt/chrome/chrome\"\n",
        "        options.add_argument(\"--headless\")\n",
        "        options.add_argument(\"--no-sandbox\")\n",
        "        options.add_argument(\"--disable-dev-shm-usage\")\n",
        "\n",
        "        driver = webdriver.Chrome(service=Service(\"/usr/bin/chromedriver\"), options=options)\n",
        "\n",
        "        driver.get(url)\n",
        "        html = driver.page_source\n",
        "        driver.quit()\n",
        "\n",
        "        # Encode as bytes\n",
        "        raw_bytes = html.encode(\"utf-8\", errors=\"ignore\")  # assume utf-8 output, ignore errors\n",
        "\n",
        "        # Detect encoding from raw bytes\n",
        "        encoding_guess = chardet.detect(raw_bytes)['encoding']\n",
        "\n",
        "        # Decode bytes into proper string using detected encoding\n",
        "        html_text = raw_bytes.decode(encoding_guess or \"utf-8\", errors=\"replace\")\n",
        "\n",
        "        # считывание html-файла и извлечение из него текста по тегу div\n",
        "        soup = BeautifulSoup(html, \"html.parser\")\n",
        "        text = \" \".join(p.get_text(strip=True) for p in soup.find_all(\"div\"))\n",
        "\n",
        "        return text[:2000]\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Selenium failed: {e}\"\n",
        "\n",
        "def read_urls(state):\n",
        "    '''\n",
        "    Функция принимает словарь state со всей информацией и добавляет в него текст,\n",
        "    обнаруженный по имеющимся url, сначала используется requests, если он не справляется,\n",
        "    то selenium\n",
        "    '''\n",
        "    texts = []\n",
        "    state.setdefault(\"web_texts\", [])\n",
        "    cutoff = len(state[\"web_texts\"])\n",
        "    for url in state[\"urls\"][cutoff:]:\n",
        "        try:\n",
        "            response = requests.get(url, timeout=10)\n",
        "\n",
        "            # подбор кодировки для корректного считывания текста\n",
        "            if response.encoding is None or response.encoding.lower() in ['iso-8859-1', 'utf-8', 'ascii']:\n",
        "                # Some servers default to iso-8859-1 even when it's wrong\n",
        "                encoding = response.apparent_encoding\n",
        "                response.encoding = encoding\n",
        "            else:\n",
        "                encoding = response.encoding\n",
        "\n",
        "            # считывание html-файла и извлечение из него текста по тегу div\n",
        "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "            text = \" \".join(p.get_text(strip=True) for p in soup.find_all(\"div\"))\n",
        "\n",
        "\n",
        "            if text == '': # проверка, справился ли requests\n",
        "                text = read_url_selenium(url)\n",
        "                texts.append(text)\n",
        "\n",
        "            else:\n",
        "                texts.append(text[:2000])  # limit for performance\n",
        "\n",
        "        except Exception as e:\n",
        "            texts.append(f\"Failed to load {url}: {e}\")\n",
        "    state.setdefault(\"web_texts\", []).extend(texts)\n",
        "    return state\n",
        "\n",
        "summary_prompt = PromptTemplate.from_template(\n",
        "'''\n",
        "Summarize the following content in under 1000 characters, focusing on key details about **{request}**:\n",
        "\n",
        "**Content:**\n",
        "{content}\n",
        "\n",
        "**Rules:**\n",
        "1. If the content is an error description, respond **only** with: `fail`\n",
        "2. Exclude meta-commentary (e.g., \"Summary is...\" or \"The content appears...\").\n",
        "3. Keep strictly under 1000 characters.''')\n",
        "\n",
        "summary_chain = summary_prompt | llm | StrOutputParser()\n",
        "\n",
        "def summarize_texts(state):\n",
        "    '''\n",
        "    Функция принимает словарь state со всей информацией\n",
        "    и добавляет в него саммари из ранее не саммаризованных веб-текстов.\n",
        "    Используется модель \"deepseek/deepseek-chat-0324-alt-fast\" с температурой 0.1\n",
        "\n",
        "    '''\n",
        "    state.setdefault('summaries', [])\n",
        "    cutoff = len(state[\"summaries\"]) # отсечка, чтобы модель работала только с новыми веб-текстами\n",
        "    summaries = [summary_chain.invoke({\"content\": text, \"request\": state['request']}) for text in state[\"web_texts\"][cutoff:]]\n",
        "    state[\"summaries\"].extend(summaries)\n",
        "    return state"
      ],
      "metadata": {
        "id": "mpY0f8SOuW3u"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decide_search(state):\n",
        "    '''\n",
        "    Функция принимает словарь state со всей информацией\n",
        "    и направляет работу графа на новый поиск, либо завершает ее\n",
        "    при наличии в state финального ответ модели либо превышении лимита на два поиска.\n",
        "    '''\n",
        "    if state[\"count\"] >= 2 or \"final_answer\" in state.keys():\n",
        "        state['continue_search'] = \"done\"\n",
        "        return state['continue_search']\n",
        "    else:\n",
        "        state['continue_search'] = \"continue\"\n",
        "        return state['continue_search']"
      ],
      "metadata": {
        "id": "xkuqvmuHuXh0"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://app.tavily.com/home limit check\n",
        "import random\n",
        "\n",
        "Tavily_key = userdata.get('Tavily')\n",
        "\n",
        "client = TavilyClient(api_key=Tavily_key)\n",
        "\n",
        "def get_tavily_urls(query: str, max_results: int = 3) -> list[str]:\n",
        "    '''\n",
        "    Функция получает на вход строку query и число ссылок max_results, которые нужно вернуть.\n",
        "    Возвращает ссылки в виде списка\n",
        "    '''\n",
        "    search_results = client.search(query=query, search_depth=\"advanced\", max_results=max_results)\n",
        "    urls = [result[\"url\"] for result in search_results[\"results\"]]\n",
        "    return urls"
      ],
      "metadata": {
        "id": "fyoUJ3KUuZuf"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search_web_tavily(state):\n",
        "    '''\n",
        "    Функция принимает словарь state со всей информацией и запускает функцию get_tavily_urls,\n",
        "    затем добавляет полученные адреса в state\n",
        "    '''\n",
        "    query = state.get(\"model_request\")\n",
        "    results = get_tavily_urls(query, max_results=2)\n",
        "    state.setdefault(\"urls\", []).extend(results)\n",
        "    state[\"count\"] = state.get(\"count\", 0) + 1\n",
        "    return state"
      ],
      "metadata": {
        "id": "QcS9qixnubvR"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decision_prompt = PromptTemplate.from_template(\n",
        "    '''\n",
        "    **Task:** Determine if the organization matches the user’s request.\n",
        "\n",
        "    **Organization Data:**\n",
        "    - Name: {name}\n",
        "    - Address: {address}\n",
        "    - Category: {category}\n",
        "    - Prices: {prices}\n",
        "    - Reviews: {reviews}\n",
        "    - Additional summaries: {summaries}\n",
        "    - Count: {count}\n",
        "\n",
        "    **User’s request: {request}**\n",
        "\n",
        "    **Rules:**\n",
        "    1. **If insufficient data**, reply with a **web search query** (<400 chars) to find missing info about given orgainzation.\n",
        "      - Format: `Query: [your search here]`\n",
        "    2. **If count == 2 or data is sufficient**, respond **only** with:\n",
        "      - `0.0` (irrelevant) or `1.0` (fully relevant).\n",
        "    3. **No extra text** in the final answer.\n",
        "    ''')\n",
        "\n",
        "decision_chain = decision_prompt | llm | StrOutputParser()\n",
        "\n",
        "def decide(state):\n",
        "    '''\n",
        "    Функция принимает словарь state со всей информацией и принимает решение,\n",
        "    достаточно ли информации для ответа.\n",
        "    Если нет, добавляет в state текст поискового запроса.\n",
        "    Если да, то добавляет в state окончательный ответ.\n",
        "    Используется модель \"deepseek/deepseek-chat-0324-alt-fast\" с температурой 0.1.\n",
        "    '''\n",
        "    final = decision_chain.invoke({\n",
        "        \"summaries\": \"\\n\\n\".join(state[\"summaries\"]),\n",
        "        \"address\": state[\"address\"],\n",
        "        \"category\": state[\"category\"],\n",
        "        \"prices\": state[\"prices\"],\n",
        "        \"reviews\": state[\"reviews\"],\n",
        "        \"name\": state[\"name\"],\n",
        "        \"request\": state[\"request\"],\n",
        "        'count': state['count']\n",
        "    })\n",
        "\n",
        "    if final == '0.0' or final == '1.0':\n",
        "        state[\"final_answer\"] = final\n",
        "        return state\n",
        "    else:\n",
        "        state[\"model_request\"] = final\n",
        "        return state"
      ],
      "metadata": {
        "id": "d8g45ETmueWl"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_graph = StateGraph(dict)\n",
        "\n",
        "agent_graph.add_node(\"search\", RunnableLambda(search_web_tavily))\n",
        "agent_graph.add_node(\"read_urls\", RunnableLambda(read_urls))\n",
        "agent_graph.add_node(\"summarize\", RunnableLambda(summarize_texts))\n",
        "agent_graph.add_node(\"decide\", RunnableLambda(decide))\n",
        "agent_graph.add_node(\"decide_search\", RunnableLambda(decide_search))\n",
        "\n",
        "# Define flow\n",
        "agent_graph.set_entry_point(\"decide\")\n",
        "\n",
        "agent_graph.add_conditional_edges(\"decide\", decide_search, {\n",
        "    \"continue\": \"search\",  # loop again\n",
        "    \"done\": END           # exit the graph\n",
        "})\n",
        "\n",
        "agent_graph.add_edge(\"search\", \"read_urls\")\n",
        "agent_graph.add_edge(\"read_urls\", \"summarize\")\n",
        "agent_graph.add_edge(\"summarize\", \"decide\")\n",
        "\n",
        "# Compile graph\n",
        "agent_graph_executor = agent_graph.compile()"
      ],
      "metadata": {
        "id": "1twhL5OougXS"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Схема графа:\n",
        "1. Информация поступает в решающую сеть decide\n",
        "2. Если decide считает, что имеющейся информации недостаточно, то она отвечает текстом поискового запроса и направляет его в модуль search\n",
        "3. Search отправляет запрос черех tavily и возвращает первые две ссылки из поиска\n",
        "4. Текст по этим ссылкам затем считывает функция read_urls. Она работает через request, а если request не считывает текст, то подключает selenium\n",
        "5. Считанный текст отправляется в модуль summarize, где модель пишет короткие саммари на основе этого текста\n",
        "6. Саммари всесте со всей остальной информацией возвращаются в decide, которая может либо еще один раз совершить поиск (возможно максимум 2 поиска для одного объекта), либо дать финальный ответ: 0.0 либо 1.0"
      ],
      "metadata": {
        "id": "ADaDEbD_xUGZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "svg = agent_graph_executor.get_graph().print_ascii()\n",
        "display(svg)  # in Jupyter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "id": "4QvHYEAZubtJ",
        "outputId": "f4465848-5b92-4cf1-e91a-ddb0c9c61a8b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                     +-----------+                 \n",
            "                     | __start__ |                 \n",
            "                     +-----------+                 \n",
            "                           *                       \n",
            "                           *                       \n",
            "                           *                       \n",
            "                      +--------+                   \n",
            "                      | decide |*                  \n",
            "                   ...+--------+ ****              \n",
            "               ....        .         ****          \n",
            "           ....           .              ***       \n",
            "         ..               .                 ****   \n",
            "+---------+          +--------+                 ** \n",
            "| __end__ |          | search |                  * \n",
            "+---------+          +--------+                  * \n",
            "                          *                      * \n",
            "                          *                      * \n",
            "                          *                      * \n",
            "                    +-----------+                * \n",
            "                    | read_urls |             ***  \n",
            "                    +-----------+            *     \n",
            "                               ***        ***      \n",
            "                                  *      *         \n",
            "                                   **  **          \n",
            "                               +-----------+       \n",
            "                               | summarize |       \n",
            "                               +-----------+       \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 hour, ~700 roubles\n",
        "# 420 tavily tokens\n",
        "from IPython.display import clear_output\n",
        "\n",
        "agent_predictions = []\n",
        "for index, organization in tqdm(eval_data.iterrows()): # eval_data\n",
        "    state = {\"model_request\": '',\n",
        "             \"request\": organization['Text'],\n",
        "             'name': organization['name'].split(';')[:2],\n",
        "             'address': organization['address'],\n",
        "             'category': organization['normalized_main_rubric_name_ru'],\n",
        "             'prices': organization['prices_summarized'],\n",
        "             'reviews': organization['reviews_summarized'],\n",
        "             'summaries': [],\n",
        "             'count': 0}\n",
        "\n",
        "    result = agent_graph_executor.invoke(state)\n",
        "    agent_predictions.append(result)\n",
        "    clear_output(wait=True)\n",
        "    '''\n",
        "    print(state)\n",
        "    print(\"Final Answer:\\n\", result[\"final_answer\"])\n",
        "    '''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GjBJJ9gkzXzC",
        "outputId": "96bf6b41-e4e9-4d88-8474-398b3576abbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "500it [1:05:48,  7.90s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gt_relevancies = eval_data['relevance'].to_list()"
      ],
      "metadata": {
        "id": "bBuDDj9QI8N1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_relevancies = []\n",
        "for i in agent_predictions:\n",
        "    agent_relevancies.append(float(i['final_answer']))"
      ],
      "metadata": {
        "id": "n5ZxfoqfI9LV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "print('Agent accuracy:', accuracy_score(gt_relevancies, agent_relevancies))\n",
        "print('Agent F1 score:', f1_score(gt_relevancies, agent_relevancies))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFoZr6zzIxtQ",
        "outputId": "3ed17b8d-6d3f-4888-d519-9d6315f5cf74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agent accuracy: 0.66\n",
            "Agent F1 score: 0.6816479400749064\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('agent_predictions_full_1.pkl', 'wb') as f:\n",
        "    pickle.dump(agent_predictions, f)"
      ],
      "metadata": {
        "id": "73nQ9g6Ko_zu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}